Below is a dropâ€‘in markdown file (ROADMAP_V2.md) you can add to the repository root.
It is written as a set of instructions that Claude Sonnet 4.0 (or any other AI pairâ€‘programmer) can follow phaseâ€‘byâ€‘phase.
Each phase ends with clear Deliverables and an Exit Checklist so you can merge, test, or revert with minimal risk.

â¸»


# Talking Objects v2 Roadmap ðŸš¦

**Goal:** ship realâ€‘time voice, persistent memory, location chat, Twitter/X syndication, and a full UI refresh without rewriting the stack.

<details>
<summary>TL;DR â€“ why this plan</summary>

* We keep the existing **Next 15 + Supabase** skeleton; replacing it would throw away functioning auth, DB, and deployment.  [oai_citation:0â€¡supabase.com](https://supabase.com/docs/guides/functions/schedule-functions?utm_source=chatgpt.com) [oai_citation:1â€¡supabase.com](https://supabase.com/blog/supabase-cron?utm_source=chatgpt.com)  
* **OpenAI TTS (`ttsâ€‘1`)** is cheap ($0.015 / 1 k chars) and already covered by our account, while **ElevenLabs** offers <300 ms latency and premium cloned voices at ~$0.0002 / char for Pro. We'll ship both behind a toggle.  [oai_citation:2â€¡community.openai.com](https://community.openai.com/t/precise-pricing-for-tts-api/634297?utm_source=chatgpt.com) [oai_citation:3â€¡platform.openai.com](https://platform.openai.com/docs/guides/text-to-speech?utm_source=chatgpt.com) [oai_citation:4â€¡elevenlabs.io](https://elevenlabs.io/pricing?utm_source=chatgpt.com) [oai_citation:5â€¡elevenlabs.io](https://elevenlabs.io/pricing/api?utm_source=chatgpt.com) [oai_citation:6â€¡elevenlabs.io](https://elevenlabs.io/blog/how-do-you-optimize-latency-for-conversational-ai?utm_source=chatgpt.com) [oai_citation:7â€¡elevenlabs.io](https://elevenlabs.io/docs/models?utm_source=chatgpt.com)  
* Memory will mirror the pattern used by ChatGPT ("facts" in a vector store plus daily summaries) and what Alexa calls *persistent attributes*.  [oai_citation:8â€¡techradar.com](https://www.techradar.com/computing/artificial-intelligence/chatgpt-can-remember-more-about-you-than-ever-before-should-you-be-worried?utm_source=chatgpt.com) [oai_citation:9â€¡developer.amazon.com](https://developer.amazon.com/en-US/docs/alexa/alexa-skills-kit-sdk-for-python/manage-attributes.html?utm_source=chatgpt.com)  
* X posting is throttled to 300 updates / 3 h per user; randomised timing via **Vercel Cron** keeps our feed natural (Hobby plan triggers anywhere in a 59â€‘minute window).  [oai_citation:10â€¡developer.twitter.com](https://developer.twitter.com/en/docs/twitter-api/v1/rate-limits?utm_source=chatgpt.com) [oai_citation:11â€¡developer.twitter.com](https://developer.twitter.com/en/docs/twitter-api/rate-limits?utm_source=chatgpt.com) [oai_citation:12â€¡vercel.com](https://vercel.com/docs/cron-jobs/usage-and-pricing?utm_source=chatgpt.com)

</details>

---

## Phase 0 â€“ Fork & Safety Net (Â½ day)

| Step | Action |
|------|--------|
| 0.1 | Fork `msanchezgrice/talkingobject â†’ talkingobjectsâ€‘v2` (GitHub UI). |
| 0.2 | Copy this file into `/ROADMAP_V2.md`. |
| 0.3 | Enable **Vercel Preview Deployments** on the fork; production domain stays on the old repo. |
| 0.4 | Tag the current main branch `v1.0.0` for instant rollback. ||

**Exit checklist**

- [ ] Preview deployment green  
- [ ] v1.0.0 git tag pushed  

---

## Phase 1 â€“ LLM Provider Abstraction (1â€“2 days)

### Tasks
1. **Create `lib/aiProvider.ts`**

```ts
export interface AIMessage { role: 'user'|'assistant'|'system'; content: string }
export interface LLMTool { name: string; schema: unknown }
export async function chatLLM(
  messages: AIMessage[],
  tools?: LLMTool[]
): Promise<AIMessage>
```

2. Replace existing lib/anthropic.ts calls with the interface above.
3. Implement provider: OpenAI
   â€¢ Use openai.chat.completions.create({ model:'gpt-4o-mini', stream:true, ... }) for text.
   â€¢ Add audio.speech.create wrapper for TTS.  
4. Feature flag (NEXT_PUBLIC_USE_ANTHROPIC=false) so we can roll back.

**Deliverables**
â€¢ lib/aiProvider.ts + tests
â€¢ Updated server actions / API routes

**Exit checklist**
â€¢ CI unit tests pass for Claude and OpenAI
â€¢ Manual chat works in preview with both flags

â¸»

## Phase 2 â€“ Voice Layer (3â€“4 days)

### Tasks
1. **POST /api/voice/:agentId**
   â€¢ Accept WAV/PCM blob â†’ Whisper (openai.audio.transcriptions.create).
   â€¢ Forward transcript to chatLLM.
   â€¢ Return stream: ReadableStream<Uint8Array> from TTS provider.
2. **Add TTS provider selector**
   â€¢ lib/tts.ts routes to openaiTTS() or elevenLabsTTS() based on agent or user preference.
   â€¢ ElevenLabs creds via env ELEVEN_API_KEY.  
3. **Frontâ€‘end**
   â€¢ Add MicButton.tsx (holdâ€‘toâ€‘talk) and audio element for playback.
   â€¢ Cache returned MP3/opus in Supabase Storage.

**Deliverables**
â€¢ Voice API route
â€¢ Mic UI
â€¢ Supabase bucket voiceresponses

**Exit checklist**
â€¢ Latency < 700 ms with OpenAI, < 350 ms with ElevenLabs on test page.  
â€¢ Fallback to text when audio disabled

â¸»

## Phase 3 â€“ Memory & Daily Summaries (3 days)

### Tasks
1. **DB migrations**

```sql
create table user_memory (
  id uuid primary key default gen_random_uuid(),
  user_id uuid references users,
  key text,
  value text,
  embedding vector(1536),
  updated_at timestamptz default now()
);

create table daily_summaries (
  agent_id uuid references agents,
  summary_date date,
  summary text,
  embedding vector(1536),
  primary key (agent_id, summary_date)
);
```

2. **Classifier hook**
   â€¢ After every user message, call miniâ€‘prompt "Is this a lasting fact?"; if yes, upsert to user_memory.
3. **Nightly summariser**
   â€¢ Supabase Edge Scheduler at 23:50 local runs SQL â†’ GPTâ€‘4o summary â†’ insert into daily_summaries.  
4. **Retrieval**
   â€¢ Preâ€‘chat: cosineâ€‘similarity top 3 user_memory + last 20 messages + yesterday's summary.

**Deliverables**
â€¢ Migration scripts in /supabase/migrations
â€¢ New lib/memory.ts
â€¢ Docs section in README

**Exit checklist**
â€¢ Memory toggle in user settings
â€¢ Summaries appear in DB next day

â¸»

## Phase 4 â€“ Explore 2.0 & Location Chat (2 days)

### Tasks
1. Add city varchar (or PostGIS point) column to agents.
2. /explore Server Component
   â€¢ Query by distinct city, group agents inside CityAccordion.
3. "Chat with this place" button links to /chat?agent=<id> preâ€‘loading system prompt with geo context.

**Exit checklist**
â€¢ Page renders â‰¤ 200 ms serverâ€‘time
â€¢ Deep link opens the right chat thread

â¸»

## Phase 5 â€“ Twitter / X Syndication (3 days)

### Tasks
1. **tweet_queue table:**

```sql
create table tweet_queue (
  id bigserial primary key,
  agent_id uuid,
  payload text,
  not_before timestamptz,
  tried int default 0
);
```

2. **/api/cron/tweet Edge Function**
   â€¢ SELECT * FROM tweet_queue WHERE not_before <= now() ORDER BY random() LIMIT 10  
   â€¢ Post via twitter-api-v2 (twoâ€‘legged bearer).  
   â€¢ Respect 300â€‘tweetsâ€‘perâ€‘3 h limit (counter in KV / table).  
3. **Queue filler**
   â€¢ After nightly summary (#Phase 3) insert 10 tweets with not_before = now() + random() * interval '6 hours'.
4. **Cron schedule**
   â€¢ Vercel Cron */15 * * * * (every 15 min) â€“ Hobby plan already adds up to 59 min jitter.  

**Exit checklist**
â€¢ Tweets visible on staging X account
â€¢ No 429 rateâ€‘limit logs

â¸»

## Phase 6 â€“ Visual Redesign (Parallel / 1 week)
â€¢ Swap Tailwind tokens, adopt shadcn/ui components.
â€¢ Run Percy visual tests to guard regressions.

â¸»

## Phase 7 â€“ QA & Launch (1 week)
â€¢ Mobile smokeâ€‘tests
â€¢ Performance budget: FCP < 1.8 s on 4G
â€¢ GDPR/CCPA privacy copy for memory feature

â¸»

## Rollback Strategy
1. Merge each phase behind a feature flag (NEXT_PUBLIC_V2_PHASE_X).
2. If metrics regress, disable flag â†’ old path activates instantly.
3. v1.0.0 tag remains deployed at v1.talkingobjects.ai for 30 days.

â¸»

## Appendix A â€“ Reference Material
â€¢ OpenAI TTS pricing & docs  
â€¢ ElevenLabs pricing & latency specs  
â€¢ Supabase Edge Function scheduler  
â€¢ ChatGPT memory announcement analysis  
â€¢ Alexa persistent attributes (DynamoDB)  
â€¢ Twitter posting limits  
â€¢ nodeâ€‘twitterâ€‘apiâ€‘v2 examples  
â€¢ Vercel Cron jitter window (Hobby)  

â¸»

Copyâ€‘paste this file into the repo, create phase-1 branch, and let Claude crank through each section. Good luck!

---

### How to use

1. **Add** `ROADMAP_V2.md` exactly as above.  
2. When you spin up Claude Sonnet 4.0 in your IDE, give it a simple prompt:

> "Please start with **Phase 1** in `ROADMAP_V2.md`, open a new branch, and push a PR when the exit checklist is green."

3. Review, merge, flip the feature flag, and move to the next phase. 